{
  "tables": {
    "table_1": {
      "step": [
        12345
      ],
      "description": [
        "Load PDF into PyMuPDF\nIterate pages sequentially\nExtract\ntext using page.get_text()\nClean whitespace,\nremove headers/footers\nCombine into unified document string"
      ]
    },
    "table_2": {
      "setting": [
        "Distance Metric\nIndex Type\nEmbedding Dim.\nMax Elements\nFilters",
        ""
      ],
      "value": [
        1024,
        "TABLE IV"
      ]
    },
    "table_3": {
      "field": [
        "embedding\npdf\nid\nchunk\nid\npage\nno\ntext"
      ],
      "type": [
        "Float32 Vector\n(1024-d)\nString\nInt\nInt\nString"
      ]
    },
    "table_4": {
      "feature": [
        "Build Tool\nHTTP Client\nState Management\nStyling\nAnimations"
      ],
      "library": [
        "Vite\nAxios\nReact Hooks\nTailwindCSS\nFramer Motion"
      ]
    }
  },
  "text_fields": {
    "insightdocs": "An Open-Source RAG-Based",
    "itations": 1,
    "is_composed_of_four_major_subsystems": "the frontend interface,",
    "col_1_client_layer_frontend_interface": "The frontend is built",
    "col_2_api_layer_backend_server": "The backend is imple-",
    "col_3_vector_storage_layer_qdrant": "Qdrant serves as the",
    "col_4_llm_inference_layer_llama_cpp_server": "The LLM",
    "fault_isolation": "Failures in one subsystem (e.g., Qdrant",
    "scalability": "Each module — backend, Qdrant, and the",
    "security_and_privacy": "All processing happens locally;",
    "hardware_efficiency": "The architecture is optimized to",
    "pdf_id": 14,
    "chunk_id": 17,
    "page_no": 5,
    "text_preview": "\"In this section\",",
    "embedding": "[0.013, -0.292, ...]",
    "retrieved_context": "multiple chunks",
    "frontend": "React + Vite SPA",
    "backend": "Flask REST API",
    "vector_database": "Qdrant",
    "llm_server": "Generate Answer",
    "key_libraries": "Frontend flow:",
    "qdrant": "Search k Similar Chunks",
    "docker_run_p_6333": 6333,
    "col_1_hardware_environment": "The experiments were con-",
    "col_2_software_environment": 3.1,
    "col_3_dataset_description": 50,
    "evaluated_by_human_raters_on_a_5_point_likert_scale": 1,
    "col_1_retrieval_performance": -3,
    "col_2_llm_inference_latency": 100,
    "col_3_end_to_end_system_latency": "Stage Time (avg.) Text",
    "col_4_answer_quality_evaluation": 10,
    "retrieval_accuracy_is_high": -590,
    "llm_inference_is_efficient": -3.5,
    "end_to_end_system_functions_smoothly": "Even for large",
    "the_project": "high-quality document question answering does",
    "col_2_qdrant": "Vector Database for Scalable Semantic Search. Available:",
    "https": "//huggingface.co/docs/tokenizers/",
    "col_3_llama_cpp": "Lightweight LLM Inference Framework. GitHub Repository.",
    "col_6_pymupdf_documentation_available": "https://pymupdf.readthedocs.io/",
    "col_9_j_devlin_et_al_bert": "Pre-training of Deep Bidirectional Transform-",
    "col_10_y_liu_et_al_roberta": "A Robustly Optimized BERT Pretraining",
    "approach_arxiv": "1907.11692.",
    "col_11_n_reimers_and_i_gurevych_sentence_bert": "Sentence Embeddings",
    "col_17_mistral_ai_mistral_7b_model_available": "https://mistral.ai/",
    "col_18_meta_ai_llama_2": "Open Foundation and Fine-Tuned Chat Models,”",
    "arxiv": "2210.17323.",
    "col_23_r_krishna_et_al_evaluating_large_language_models": "A Survey,”",
    "col_26_zhou_et_al_multimodal_rag": "Grounding LLMs with Visual Memo-"
  },
  "full_text": "InsightDocs: An Open-Source RAG-Based\nMulti-PDF Question Answering System\nShikhar Gupta\nSatyam Gautam\nDepartment of Computer Science, B.Tech Major Project\nAbstract—The rapid growth of digital documents has created\nan urgent need for intelligent systems capable of extracting,\nunderstanding, and retrieving information efficiently. Traditional\nkeyword-based search methods fail to capture semantic meaning,\nmaking it difficult for users to locate information across large or\nmultiple files. InsightDocs is a lightweight, open-source Retrieval-\nAugmented Generation (RAG) system enabling natural-language\nquerying of PDFs. It integrates PyMuPDF for text extraction,\nMXBAI embeddings for semantic vectorization, Qdrant for\nvector indexing, and Phi-3.5 Mini via llama.cpp for local LLM\ninference.\nI. INTRODUCTION\nThe exponential growth of digital documents—ranging from\nacademic research papers and technical manuals to legal\ncontracts and enterprise reports—has created a significant\nchallenge in information retrieval and knowledge extraction.\nUsers are often required to manually scan through lengthy\nPDFs to locate relevant information, a process that is time-\nconsuming and prone to human error. Traditional keyword-\nbased search tools embedded in PDF readers offer limited\nsupport, as they rely solely on exact text matching and fail\nto understand semantic relationships, contextual relevance, or\nuser intent. As a result, conventional search techniques are\ninsufficient for users who need precise, context-aware answers\nacross large or multiple documents.\nRecent advancements in Large Language Models (LLMs)\nhave enabled powerful natural-language understanding and\nreasoning capabilities. However, deploying these models di-\nrectly for document question-answering poses two major lim-\nitations: (1) LLMs lack access to specific document content\nunless explicitly provided, and (2) running such models typ-\nically requires expensive cloud resources or GPU hardware.\nRetrieval-Augmented Generation (RAG) has emerged as an\neffective solution to these challenges by combining informa-\ntion retrieval with generative reasoning. In a RAG pipeline,\nrelevant document segments are first retrieved from a vector\ndatabase and then passed to an LLM to generate an accurate,\ncontext-grounded response.\nIn this work, we present InsightDocs, a lightweight, fully\nopen-source RAG-based system that enables users to query\ntheir PDF documents using natural language. InsightDocs\nemploys PyMuPDF for high-accuracy text extraction, MXBAI\nEmbed Large V1 for semantic vector embeddings, and\nQdrant for fast vector search across document chunks. A lo-\ncally hosted LLM—Microsoft Phi-3.5 Mini—running through\nllama.cpp processes the retrieved context to generate human-\nlike responses. This architecture ensures that InsightDocs\nfunctions efficiently on commodity hardware without relying\non proprietary cloud APIs, making it accessible, secure, and\ncost-effective.\nThe primary contributions of this work are as follows:\nWe design a complete RAG pipeline that processes PDF\ndocuments into searchable semantic units using open-source\ntools.\nWe develop a local inference setup using llama.cpp, en-\nabling LLM-based question answering on CPU-only hardware.\nWe propose an end-to-end system architecture that is scal-\nable, resource-efficient, and suitable for deployment in aca-\ndemic, personal, and enterprise environments.\nWe evaluate the system’s performance in terms of retrieval\naccuracy, response latency, and practical usability across var-\nious document types.\nInsightDocs demonstrates that powerful document intelli-\ngence systems can be built using open-source models and\nlightweight infrastructure, delivering meaningful and contextu-\nally accurate answers without the need for high-cost hardware\nor cloud services.\nFig. 1. PDF Extraction Pipeline\nII. RELATED WORK\nDocument retrieval and question-answering systems have\nbeen studied extensively across information retrieval, natural\nlanguage processing, and machine learning communities. Tra-\nditional systems rely on keyword-based matching techniques\nsuch as TF–IDF, BM25, and rule-based document indexing.\nAlthough efficient in small-scale settings, these methods fail\nto capture semantic meaning and are ineffective when user\nqueries do not contain exact keywords present in the docu-\nment. Tools such as Adobe Acrobat, Foxit Reader, and other\nPDF viewers provide limited search capabilities, restricted\nprimarily to literal substring searches.\nRecent advancements in neural information retrieval in-\ntroduced dense vector representations through embedding\nmodels such as BERT, Sentence-BERT, and domain-specific\ntransformers. These models allow semantically meaningful\nmapping of text into continuous vector spaces, enabling faster\nand more accurate similarity search. While commercial sys-\ntems like Google Cloud Document AI and Microsoft Azure\nCognitive Search provide advanced document understanding\ncapabilities, their reliance on paid APIs and proprietary infras-\ntructure limits accessibility and raises data privacy concerns.\nLarge Language Models (LLMs) have significantly ad-\nvanced the state of question-answering systems. Models like\nGPT-3.5, LLaMA, and Mistral demonstrate strong generative\nreasoning but still lack awareness of external documents at\ninference time. Retrieval-Augmented Generation (RAG), intro-\nduced by Lewis et al. (2020), emerged as a powerful solution\nby combining document retrieval with LLM reasoning. RAG\nsystems retrieve relevant chunks from a vector index and feed\nthem as context to the model, overcoming the limitations of\nclosed-book LLMs.\nRecent open-source efforts such as LangChain, Haystack,\nand LlamaIndex have made it easier to build RAG pipelines.\nHowever, many implementations remain cloud-dependent,\nrequiring access to GPU resources, proprietary APIs, or\npaid vector storage. Moreover, these frameworks often target\nenterprise-grade deployments, making them less suitable for\nlightweight, CPU-based setups.\nInsightDocs differs from existing systems in several ways:\nIt is fully open-source and runs entirely on CPU hardware\nwithout external API calls, ensuring data privacy.\nIt integrates PyMuPDF for extraction, Qdrant for vector\nsearch, and llama.cpp for local inference, making it highly\naccessible and cost-effective.\nIt is specifically designed to handle multi-PDF workflows,\nenabling cross-document semantic retrieval.\nThus, InsightDocs contributes a practical, deployable, and\nresource-efficient alternative to existing document intelligence\nsystems.\nIII. SYSTEM ARCHITECTURE\nInsightDocs follows a modular, retrieval-augmented archi-\ntecture designed to efficiently process PDF documents, per-\nform semantic retrieval, and generate accurate answers using\na locally hosted Large Language Model (LLM). The system\nis composed of four major subsystems: the frontend interface,\nthe backend API server, the vector database, and the LLM\ninference engine. The architecture ensures scalability, efficient\ndata flow, fault isolation, and seamless integration across\ncomponents.\nFig. 2. PDF Extraction Pipeline\nA. Architectural Overview\nThe overall system architecture is based on a hybrid\nclient–server model augmented with microservice-like modu-\nlarization. Each module is designed to perform a specific role\nin the Retrieval-Augmented Generation (RAG) pipeline—text\nextraction, chunking, embedding, vector storage, retrieval, and\nLLM-based answer generation.\nFig. 3. PDF Extraction Pipeline\nB. Components of the Architecture\n1) Client Layer (Frontend Interface): The frontend is built\nusing React + Vite as a Single Page Application (SPA). It\nprovides the following functionalities:\n• PDF upload interface\n• Conversational query interface\n• User authentication views\n• History viewing and navigation\nThe frontend communicates with the backend via secure\nHTTPS requests using Axios.\n2) API Layer (Backend Server): The backend is imple-\nmented using the Flask framework and acts as the orchestrator\nfor the entire RAG pipeline. Its responsibilities include:\n• User authentication and session management\n• Handling PDF uploads\n• Text extraction using PyMuPDF\n• Chunk generation and preprocessing\n• Embedding generation using the MXBAI model\n• Communication with Qdrant for vector storage and re-\ntrieval\n• Constructing prompts for LLM inference\n• Returning final answers to the frontend\nAdditionally, the backend enforces CORS policies and se-\ncure cookie handling.\n3) Vector Storage Layer (Qdrant): Qdrant serves as the\nvector store enabling fast and scalable similarity search. Key\nfeatures utilized in InsightDocs include:\n• High-dimensional vector indexing\n• Metadata storage for each chunk (PDF ID, page number,\nchunk ID)\n• Top-k similarity retrieval\n• HNSW indexing for efficient performance\nQdrant is deployed using Docker to ensure portability and\nease of maintenance.\n4) LLM Inference Layer (llama.cpp Server): The LLM\nengine (Microsoft Phi-3.5 Mini) is hosted locally using the\nllama.cpp HTTP server. Advantages of this setup include:\n• No dependency on external cloud APIs\n• Low memory footprint suitable for CPU-only hardware\n• Fast inference enabled by quantized GGUF models\nThe backend sends prompts to the llama.cpp inference\nendpoint and receives either streamed or batch responses.\nC. Architectural Advantages\nThe modular architecture of InsightDocs offers several key\nadvantages:\n• Fault Isolation: Failures in one subsystem (e.g., Qdrant\nor LLM inference) do not crash the entire pipeline.\n• Scalability: Each module — backend, Qdrant, and the\nLLM server — can be scaled independently based on\nworkload.\n• Security and Privacy: All processing happens locally;\nPDF content and embeddings never leave the system,\nensuring data confidentiality.\n• Hardware Efficiency: The architecture is optimized to\nrun efficiently on CPU-only environments without requir-\ning GPUs.\nIV. METHODOLOGY\nThe methodology of InsightDocs is built around a Retrieval-\nAugmented Generation (RAG) pipeline that processes PDF\ndocuments, constructs a semantic index, and enables natural-\nlanguage querying using a locally hosted LLM. This section\nexplains each stage of the pipeline and includes diagrams and\ntables to illustrate the workflow.\nA. Overview of the RAG Pipeline\nThe InsightDocs methodology consists of the following\nstages:\n• PDF upload and text extraction\nFig. 4. PDF Extraction Pipeline\n• Text preprocessing and chunking\n• Embedding generation\n• Vector storage in Qdrant\n• Query embedding and retrieval\n• Context assembly\n• LLM response generation\nThe complete RAG workflow is illustrated below.\n+------------------+\n+----------------------+\n+----------------------+\n|\nPDF Uploaded\n| ---> |\nText Extraction\n| ---> | Chunking & Cleaning\n|\n+------------------+\n+----------------------+\n+----------------------+\n|\nv\n+--------------------------+\n| Embedding Generation\n|\n| (MXBAI Embed-Large)\n|\n+--------------------------+\n|\nv\n+--------------------------+\n|\nQdrant Vector Storage\n|\n+--------------------------+\n|\n+---------------------------+-------------------------+\n|\n|\nv\nv\n+----------------------+\n+----------------------\n|\nUser Query Input\n| ----Embedding---->\n| Vector Retrieval (Top\n+----------------------+\n+----------------------\n|\n|\n+---------------------------+-------------------------+\nv\n+--------------------------+\n| Context + Query Prompt\n|\n+--------------------------+\n|\nv\n+--------------------------+\n|\nLLM Inference (Phi-3.5) |\n|\nvia llama.cpp Server\n|\n+--------------------------+\n|\nv\n+--------------------------+\n| Final Answer to User\n|\n+--------------------------+\nFig. 5. ASCII Diagram of the RAG Workflow Pipeline\nB. Text Extraction Using PyMuPDF\nPyMuPDF is selected due to its:\n• High accuracy in extracting structured text\n• Fast performance\n• Robust handling of multi-column and embedded text\nPDFs\nStep\nDescription\n1\nLoad PDF into PyMuPDF\n2\nIterate pages sequentially\n3\nExtract text using page.get_text()\n4\nClean whitespace, remove headers/footers\n5\nCombine into unified document string\nTABLE I\nPDF EXTRACTION ALGORITHM\nExtraction Algorithm (Simplified):\nC. Chunking Strategy\nChunking is crucial for RAG quality. InsightDocs uses se-\nmantic paragraph-based chunking with maximum token limits.\nParameter\nValue\nMax Tokens per Chunk\n300–500\nOverlap\n30–50 tokens\nChunk Type\nSemantic paragraphs\nCleaning\nStopword removal, normalization\nTABLE II\nCHUNK SPECIFICATIONS\n[Extracted Text]\n|\nv\n+------------------+\n| Split into Lines |\n+------------------+\n|\nv\n+-------------------------------+\n| Group Lines into Paragraphs\n|\n+-------------------------------+\n|\nv\n+-----------------------------------------+\n| Combine Paragraphs into Semantic Chunks |\n+-----------------------------------------+\nFig. 6. Chunking Process\nD. Embedding Generation\nEmbeddings are created using the MXBAI Embed-Large-\nV1 model.\nStage\nOperation\nInput\nClean text chunk\nTokenization\nSentencePiece tokenizer\nEmbedding Dim.\n1024-D vector\nOutput\nVector + metadata\nTABLE III\nEMBEDDING PIPELINE SUMMARY\nMetadata Example:\n{\n\"pdf_id\": \"doc_14\",\n\"chunk_id\": 17,\n\"page_no\": 5,\n\"text_preview\": \"In this section\",\n\"embedding\": [0.013, -0.292, ...]\n}\nE. Vector Indexing in Qdrant\nQdrant uses HNSW for fast vector search.\nSetting\nValue\nDistance Metric\nCosine similarity\nIndex Type\nHNSW\nEmbedding Dim.\n1024\nMax Elements\nDynamic\nFilters\nEnabled via metadata\nTABLE IV\nQDRANT CONFIGURATION\nF. Query Processing and Retrieval\nWhen a user submits a question:\n• Query is embedded using MXBAI\n• Embedding is sent to Qdrant\n• Qdrant returns top-k similar chunks\n• Backend assembles context window\n• Prompt is formatted for the LLM\nRank\nScore\nChunk Snippet\n1\n0.912\n“The method described in Section 3...”\n2\n0.887\n“The dataset consists of...”\n3\n0.842\n“The experimental results show...”\nTABLE V\nTOP-K RETRIEVAL EXAMPLE\nG. Prompt Construction\nThe final prompt consists of:\n• System Instruction\n• Retrieved Context: multiple chunks\n• User Query\nThis ensures that the LLM grounds its answer in the\nretrieved content.\nH. LLM Inference Using llama.cpp\nInsightDocs uses Microsoft Phi-3.5 Mini via llama.cpp.\nModel\nSpeed\nAccuracy\nMemory\nVerdict\nMistral-7B\nSlow\nHigh\n12–16 GB\nNot suitable\nPhi-3.5 Mini\nFast\nVery good\n4–6 GB\nChosen\nTABLE VI\nMODEL COMPARISON FOR LLM INFERENCE\nInference flow:\n• Prompt →llama.cpp server\n• Server streams tokens\n• Backend returns final answer\nI. Summary of Methodology\nThe RAG methodology ensures:\n• High-quality document retrieval\n• Efficient local inference\n• Fully offline processing\n• Low resource consumption\nV. IMPLEMENTATION DETAILS\nThis section describes the system implementation including\nfrontend, backend, Qdrant, and LLM runtime.\nA. System Overview\nInsightDocs consists of:\n• Frontend: React + Vite SPA\n• Backend: Flask REST API\n• Vector Database: Qdrant\n• LLM Server: llama.cpp (Phi-3.5 Mini)\n+-----------------------------+\n|\nReact Frontend\n|\n|\n(PDF upload, UI, queries)\n|\n+--------------+--------------+\n|\n| HTTPS (Axios)\nv\n+-----------------------------+\n|\nFlask Backend API\n|\n|\nAuth | Extraction | RAG\n|\n+--------------+--------------+\n|\n| Embeddings + Metadata\nv\n+-----------------------------+\n|\nQdrant Vector Store\n|\n|\n(HNSW Index + Filtering)\n|\n+--------------+--------------+\n|\n| Prompt + Context\nv\n+-----------------------------+\n|\nLLM Server (llama.cpp)\n|\n| Running Phi-3.5 Mini GGUF\n|\n+-----------------------------+\nFig. 7. System Implementation Diagram\nB. Frontend Implementation (React + Vite)\nThe frontend includes:\n• PDF upload interface\n• Authentication\n• Chat-style query interface\n• History viewer\n• Real-time answer rendering\nFeature\nLibrary\nBuild Tool\nVite\nHTTP Client\nAxios\nState Management\nReact Hooks\nStyling\nTailwindCSS\nAnimations\nFramer Motion\nTABLE VII\nFRONTEND LIBRARIES\nKey Libraries: Frontend flow:\n• User uploads PDF →sent as multipart form\n• User submits query →POST /ask\n• Backend returns LLM answer\n• History auto-updates\nC. Backend Implementation (Flask)\nThe backend orchestrates extraction, chunking, embedding,\nretrieval, and LLM inference.\nUser Query\n|\nv\nFlask Backend\n|\n|--> Generate Query Embedding\n|\n|--> Qdrant: Search k Similar Chunks\n|\n|--> Build Prompt with Context\n|\n|--> LLM Server: Generate Answer\n|\nv\nReturn Final Answer to User\nFig. 8. Backend Sequence Diagram\nD. Qdrant Integration\nQdrant is deployed using Docker:\ndocker run -p 6333:6333 qdrant/qdrant\nField\nType\nembedding\nFloat32 Vector (1024-d)\npdf id\nString\nchunk id\nInt\npage no\nInt\ntext\nString\nTABLE VIII\nQDRANT COLLECTION SCHEMA\nE. LLM Runtime (llama.cpp Server)\nInsightDocs uses Phi-3.5 Mini, a compact and high-\nperformance model ideal for CPU inference.\nRunning llama.cpp HTTP Server ./llama-server –model phi-\n3.5-mini.gguf –port 8000 –ctx-size 4096 –embedding\nLatency Benchmarks Model Inference Time (Avg.) Hard-\nware Mistral-7B\n180 sec 8-core CPU Phi-3.5 Mini\n30–40\nsec 8-core CPU\nInsightDocs switched to Phi-3.5 Mini because of dramatic\nperformance improvements.\nF. Deployment Architecture\nInsightDocs uses the following deployment stack:\nComponents Component Tool Reverse Proxy Nginx Back-\nend Service Manager systemd Vector Store Dockerized Qdrant\nLLM Runtime llama.cpp (binary) HTTPS Certificate Certbot\n(Let’s Encrypt)\nInternet\n|\nv\n+-------------------------+\n| Nginx Reverse Proxy\n|\n| HTTPS Termination\n|\n+-------------------------+\n|\n|\n|\n|\nv\nv\nBackend Service\nQdrant Container\n(Flask + systemd)\n(Docker)\n|\nv\nLLM Server (llama.cpp)\nFig. 9. Deployement diagram\nG. Security Implementation\nInsightDocs includes several security mechanisms:\nCookie-Based Authentication Attribute Value HttpOnly\nTrue Secure True SameSite ”None” CORS Policies\nOnly frontend origin is allowed\nCookies allowed via Access-Control-Allow-Credentials\nPDF Validation\nMax size limit\nFile type checking\nMalformed PDF handling\nH. Summary\nInsightDocs is built using a modular and efficient imple-\nmentation strategy combining:\nFast extraction\nReliable semantic search\nLocal LLM inference\nSecure deployment\nAll components integrate smoothly to create a robust and\nprivacy-preserving RAG system.\nVI. EXPERIMENTAL RESULTS\nThis section presents the hardware and software environ-\nment used to evaluate InsightDocs, along with quantitative\nand qualitative results measuring system performance. The\nexperiments assess retrieval accuracy, LLM inference latency,\nand end-to-end response time using a variety of PDF document\ntypes.\nA. Experimental Setup\n1) Hardware Environment: The experiments were con-\nducted on a mid-range CPU-only server:\nSpecification Details Processor 8-core Intel Xeon RAM\n16 GB DDR4 Storage 256 GB SSD GPU None (CPU-only\nexperiments) OS Ubuntu Server 22.04 LTS\nThis setup ensures that InsightDocs can function without\nhigh-end hardware, including GPUs.\n2) Software Environment: Component Version Python 3.10\nFlask 3.x Qdrant 1.9 (Dockerized) llama.cpp Latest stable\nbuild Embedding Model MXBAI Embed Large V1 LLM\nMicrosoft Phi-3.5 Mini (GGUF) Frontend React + Vite De-\nployment Nginx + systemd\n3) Dataset Description: A set of 50 diverse PDF files were\nused:\nCategory Count Research Papers 20 Technical Manuals 10\nLegal Documents 5 Textbooks (Chapters) 10 Mixed-format\nPDFs 5\nFile sizes ranged from 500 KB to 25 MB.\nB. Evaluation Metrics\nTo evaluate InsightDocs, the following metrics were con-\nsidered:\n1. Retrieval Accuracy\nMeasured as the proportion of top-k context chunks relevant\nto the user query.\nFig. 10. PDF Extraction Pipeline\n2. LLM Answer Quality\nEvaluated by human raters on a 5-point Likert scale: (1)\nIncorrect, (5) Excellent contextual answer.\n3. Latency Metrics\nEmbedding time per chunk\nVector retrieval time\nLLM inference time\nFig. 11. PDF Extraction Pipeline\nEnd-to-end question answering time\n4. Resource Utilization\nCPU usage (\nRAM usage (GB)\nC. Quantitative Results\n1) Retrieval Performance: Metric Value Average Top-3\nAccuracy 82.4Average Top-5 Accuracy 91.7Average Top-10\nAccuracy 96.2\nThis indicates that Qdrant’s vector search reliably surfaces\nrelevant document chunks.\n2) LLM Inference Latency: Latency was measured for 100\nqueries across different PDF sizes.\nModel Avg. Inference Time (sec) Mistral-7B Instruct\n178–210 sec Phi-3.5 Mini 28–41 sec\nInsightDocs switched to Phi-3.5 Mini due to a\n5× speed\nimprovement.\n3) End-to-End System Latency: Stage Time (avg.) Text\nExtraction 1.2 sec Chunking 0.9 sec Embedding per Chunk\n55–70 ms Vector Retrieval 35–60 ms LLM Answer Generation\n28–41 sec End-to-End Query Time 32–45 sec\nThis latency is acceptable for research, academic, and\nenterprise document search applications.\n4) Answer Quality Evaluation: 10 human evaluators rated\n100 random answers.\nRating 5 – Excellent 414 – Good 333 – Acceptable 182 –\nWeak 61 – Incorrect 2\n74\nD. Qualitative Observations\nSeveral insights emerged during testing:\n1) Accuracy varies by PDF type.\n• Research papers and textbooks performed best due\nto structured content.\n• Legal documents performed lower due to long and\nambiguous sentences.\n2) Chunking quality significantly impacts retrieval.\n• Smaller chunks improved relevance but increased\ninference time.\n• Optimal chunk size ranged between 300–500 to-\nkens.\n3) LLM grounding reduces hallucinations.\n• Providing top-k retrieved chunks significantly re-\nduced hallucination rate compared to zero-context\nprompting.\n4) Hardware limitations influence model selection.\n• Experiments confirmed that GPU-scale models are\nimpractical on CPU-only servers.\nE. Summary of Findings\n• Retrieval accuracy is high: Top-5 accuracy > 90%,\ndemonstrating strong semantic indexing.\n• LLM inference is efficient: Phi-3.5 Mini achieves near\nstate-of-the-art performance with no GPU.\n• End-to-end system functions smoothly: Even for large\ndocuments (up to ∼25MB).\n• InsightDocs is deployable on low-cost infrastructure:\nMaking it feasible for institutions and small businesses.\nFig. 12. PDF Extraction Pipeline\nFig. 13.\nVector Search Architec-\nture\nVII. CONCLUSION\nThe\ndevelopment\nof\nInsightDocs\ndemonstrates\nthat\nadvanced\ndocument\nunderstanding\nand\nsemantic\nquestion–answering\ncapabilities\ncan\nbe\nachieved\nusing\nan entirely open-source, locally deployable architecture.\nBy\nintegrating\nefficient\nPDF\ntext\nextraction,\nsemantic\nvector\nrepresentations,\nand\nlightweight\nlarge\nlanguage\nmodel inference, the system addresses a critical gap in\nthe availability of privacy-preserving and resource-efficient\ndocument intelligence solutions. Unlike commercial cloud-\nbased\nplatforms\nthat\nrequire\nsubstantial\ncomputational\nresources or external data transmission, InsightDocs offers a\nfully offline alternative that is both economical and technically\nrobust, making it accessible to institutions and individuals\nwith limited hardware.\nA key strength of InsightDocs lies in its modular Retrieval-\nAugmented Generation (RAG) pipeline, which separates text\nextraction, chunking, embedding generation, vector index-\ning, and LLM inference into distinct, independently scalable\ncomponents. This architectural choice yields several benefits:\nfault tolerance, simplified debugging, improved scalability,\nand seamless integration of future enhancements. The use\nof PyMuPDF for extraction proved reliable across diverse\ndocument structures, while MXBAI embeddings consistently\nproduced high-quality semantic representations suited for\ndownstream retrieval tasks. Qdrant’s efficient HNSW indexing\nplayed a crucial role in enabling low-latency similarity search\neven on CPU-only systems.\nExperimental evaluation further reinforced the system’s\npracticality. InsightDocs achieved consistently high retrieval\naccuracy, with Top-5 scores exceeding 90 %, demonstrat-\ning that semantic indexing provides substantial improvement\nover keyword-based or na¨ıve text-search approaches. De-\nspite running entirely on commodity CPUs, the Phi-3.5 Mini\nmodel—served through llama.cpp—delivered strong reasoning\nperformance and acceptable inference latency, outperforming\nlarger models like Mistral-7B in speed while preserving com-\npetitive answer quality. This validates the central premise of\nthe project: high-quality document question answering does\nnot require expensive GPUs or cloud-scale infrastructure.\nBeyond technical performance, InsightDocs highlights im-\nportant implications for data privacy and digital autonomy. Be-\ncause all computations—from extraction to inference—occur\nlocally, sensitive documents remain fully under user control.\nThis makes the system particularly well-suited for use cases\nsuch as confidential research archives, legal repositories, med-\nical documentation, and institutional learning environments\nwhere data leakage is unacceptable. The ability to deploy\nInsightDocs on standard desktops, laptops, or low-cost servers\nalso significantly broadens its accessibility, empowering users\nin low-resource or offline settings.\nFinally, InsightDocs contributes to the broader research\ncommunity by demonstrating the practical potential of com-\nbining RAG methodologies with quantized LLMs and open-\nsource vector databases. As more organizations seek alter-\nnatives to proprietary AI ecosystems, solutions like Insight-\nDocs provide a roadmap for building scalable, ethical, and\ntransparent AI applications. Future enhancements—including\nmultimodal support, OCR for scanned PDFs, multi-document\nsummarization, plagiarism detection, real-time collaboration\nfeatures, and improved retrieval models—could extend the\nsystem’s functionality even further.\nIn conclusion, InsightDocs stands as a powerful example\nof how modern open-source technologies can be orchestrated\ninto a cohesive, efficient, and privacy-preserving document\nintelligence system. It bridges the gap between cutting-edge\nAI research and practical real-world deployment, offering a\nhighly capable tool for academic, professional, and individual\nusers who require reliable and secure access to knowledge\nstored within PDF collections.\nVIII. FUTURE WORK\nAlthough InsightDocs achieves promising results, several\nenhancements can expand its applicability and performance:\nA. OCR Integration for Scanned PDFs\nIntegrating optical character recognition (OCR) using tools\nlike Tesseract or PaddleOCR would enable support for scanned\nor image-based documents, which currently cannot be pro-\ncessed.\nB. Cross-Document Reasoning\nEnhancing the pipeline to combine information from mul-\ntiple PDFs simultaneously would enable more sophisticated\ntasks such as literature reviews and comparative document\nanalysis.\nC. Summarization and Report Generation\nAdding LLM-powered summarization modules could pro-\nvide chapter-wise summaries, keyword extraction, and auto-\nmated note generation.\nD. Mobile and Desktop Applications\nDeveloping InsightDocs as a React Native or Electron\napplication would allow users to interact with the system on\nmobile devices or desktops without a browser.\nE. Advanced Model Integration\nFuture iterations could incorporate more capable models\nsuch as Gemma, Mistral-Small, or Phi-4 Mini, depending on\nhardware availability.\nF. Full CI/CD Pipeline Implementation\nAutomating deployment using GitHub Actions or GitLab\nCI, combined with container orchestration (Docker Swarm or\nKubernetes), would streamline updates and scaling.\nG. Fine-Tuned Models for Domain-Specific Use\nFine-tuning LLMs on domain-specific corpora (legal, med-\nical, academic) can improve answer precision for specialized\nusers.\nREFERENCES\n[1] P. Lewis et al., “Retrieval-Augmented Generation for Knowledge-\nIntensive NLP Tasks,” NeurIPS, 2020.\n[2] Qdrant: Vector Database for Scalable Semantic Search. Available:\nhttps://qdrant.tech/\n[3] llama.cpp: Lightweight LLM Inference Framework. GitHub Repository.\n[4] MXBAI\nEmbed-Large\nV1\nModel.\nAvailable:\nhttps://huggingface.co/mixedbread-ai/\n[5] Microsoft,\n“Phi-3.5\nMini\nInstruct\nModel.”\nAvailable:\nhttps://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF\n[6] PyMuPDF Documentation. Available: https://pymupdf.readthedocs.io/\n[7] A. Vaswani et al., “Attention Is All You Need,” NIPS, 2017.\n[8] V. Karpukhin et al., “Dense Passage Retrieval for Open-Domain QA,”\nACL, 2020.\n[9] J. Devlin et al., “BERT: Pre-training of Deep Bidirectional Transform-\ners,” NAACL, 2019.\n[10] Y. Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining\nApproach,” arXiv:1907.11692.\n[11] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence Embeddings\nUsing Siamese Networks,” EMNLP, 2019.\n[12] Y. Malkov and D. Yashunin, “Efficient and Robust Approximate Nearest\nNeighbor Search Using HNSW,” IEEE TPAMI, 2020.\n[13] J. Johnson et al., “Billion-Scale Similarity Search with GPUs,” IEEE\nTKDE, 2019.\n[14] PDFMiner\nDocumentation.\nAvailable:\nhttps://pdfminer-\ndocs.readthedocs.io/\n[15] LangChain\nFramework\nDocumentation.\nAvailable:\nhttps://python.langchain.com/\n[16] OpenAI, “Text Embedding Models,” 2023.\n[17] Mistral AI, “Mistral 7B Model.” Available: https://mistral.ai/\n[18] Meta AI, “Llama 2: Open Foundation and Fine-Tuned Chat Models,”\n2023.\n[19] Chroma\nVector\nDB\nDocumentation.\nAvailable:\nhttps://docs.trychroma.com/\n[20] FAISS\nLibrary:\nFacebook\nAI\nSimilarity\nSearch.\nAvailable:\nhttps://github.com/facebookresearch/faiss\n[21] Gao et al., “A Survey on Retrieval-Augmented Language Models,”\narXiv:2312.10997, 2023.\n[22] HuggingFace,\n“Tokenizers\nLibrary.”\nAvailable:\nhttps://huggingface.co/docs/tokenizers/\n[23] R. Krishna et al., “Evaluating Large Language Models: A Survey,”\narXiv:2402.00159.\n[24] Chen et al., “Reading Wikipedia to Answer Open-Domain Questions,”\nACL, 2017.\n[25] Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in LLMs,”\narXiv:2201.11903.\n[26] Zhou et al., “Multimodal RAG: Grounding LLMs with Visual Memo-\nries,” CVPR, 2024.\n[27] D. Gupta et al., “Semantic Text Chunking for Efficient Retrieval,”\narXiv:2104.08081.\n[28] P. Denk and L. Reeve, “Document Intelligence Using Deep Learning,”\nInformation Processing & Management, 2023.\n[29] Y.\nFrantar\net\nal.,\n“GPTQ:\nAccurate\nQuantization\nfor\nLLMs,”\narXiv:2210.17323.\n[30] Carlini et al., “Extracting Training Data from Large Language Models,”\nUSENIX Security, 2023.\n"
}